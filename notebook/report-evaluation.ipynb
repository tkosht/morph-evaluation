{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeCab, SentencePiece の精度評価\n",
    "\n",
    "- 評価データセット：ldcc\n",
    "- 評価方法：pipeline\n",
    "    - ../model/\n",
    "        - pipe-jptokenizermecab.gz\n",
    "        - pipe-jptokenizersentencepiece.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from classify_ldcc import DocRecord, DatasetLdcc\n",
    "from classify_ldcc import JpTokenizerMeCab, JpTokenizerSentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelineの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from classify_ldcc import ident_tokener, SparsetoDense, Transer\n",
    "os.chdir(\"../\")\n",
    "pipe_mecab = joblib.load(\"model/pipe-jptokenizermecab.gz\")\n",
    "pipe_sentencepiece = joblib.load(\"model/pipe-jptokenizersentencepiece.gz\")\n",
    "os.chdir(\"notebook/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tokenizer',\n",
       "                 <classify_ldcc.JpTokenizerMeCab object at 0x7f0988e4f978>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf...\n",
       "                 LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                colsample_bytree=1.0, importance_type='gain',\n",
       "                                learning_rate=0.1, max_depth=-1,\n",
       "                                min_child_samples=20, min_child_weight=0.001,\n",
       "                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                                num_class=9, num_leaves=31, objective='softmax',\n",
       "                                random_state=None, reg_alpha=0.0,\n",
       "                                reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "                                subsample_for_bin=200000, subsample_freq=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tokenizer',\n",
       "                 <classify_ldcc.JpTokenizerSentencePiece object at 0x7f090dc856a0>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, sm...\n",
       "                 LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                colsample_bytree=1.0, importance_type='gain',\n",
       "                                learning_rate=0.1, max_depth=-1,\n",
       "                                min_child_samples=20, min_child_weight=0.001,\n",
       "                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                                num_class=9, num_leaves=31, objective='softmax',\n",
       "                                random_state=None, reg_alpha=0.0,\n",
       "                                reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "                                subsample_for_bin=200000, subsample_freq=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>cpu_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942560</td>\n",
       "      <td>62.041894</td>\n",
       "      <td>296.817731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>94.967452</td>\n",
       "      <td>549.502217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953415</td>\n",
       "      <td>61.877692</td>\n",
       "      <td>294.313050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951153</td>\n",
       "      <td>95.696913</td>\n",
       "      <td>551.355981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947535</td>\n",
       "      <td>61.406188</td>\n",
       "      <td>291.654511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tokenizer  train_acc  valid_acc  elapsed_time    cpu_time\n",
       "0          JpTokenizerMeCab        1.0   0.942560     62.041894  296.817731\n",
       "1  JpTokenizerSentencePiece        1.0   0.953867     94.967452  549.502217\n",
       "2          JpTokenizerMeCab        1.0   0.953415     61.877692  294.313050\n",
       "3  JpTokenizerSentencePiece        1.0   0.951153     95.696913  551.355981\n",
       "4          JpTokenizerMeCab        1.0   0.947535     61.406188  291.654511"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_csv = \"../data/result.csv\"\n",
    "columns = [\"tokenizer\", \"train_acc\", \"valid_acc\", \"elapsed_time\", \"cpu_time\"]\n",
    "df = pandas.read_csv(result_csv, header=None, names=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回数情報を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>cpu_time</th>\n",
       "      <th>times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.942560</td>\n",
       "      <td>62.041894</td>\n",
       "      <td>296.817731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>94.967452</td>\n",
       "      <td>549.502217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953415</td>\n",
       "      <td>61.877692</td>\n",
       "      <td>294.313050</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951153</td>\n",
       "      <td>95.696913</td>\n",
       "      <td>551.355981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947535</td>\n",
       "      <td>61.406188</td>\n",
       "      <td>291.654511</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tokenizer  train_acc  valid_acc  elapsed_time    cpu_time  \\\n",
       "0          JpTokenizerMeCab        1.0   0.942560     62.041894  296.817731   \n",
       "1  JpTokenizerSentencePiece        1.0   0.953867     94.967452  549.502217   \n",
       "2          JpTokenizerMeCab        1.0   0.953415     61.877692  294.313050   \n",
       "3  JpTokenizerSentencePiece        1.0   0.951153     95.696913  551.355981   \n",
       "4          JpTokenizerMeCab        1.0   0.947535     61.406188  291.654511   \n",
       "\n",
       "   times  \n",
       "0      1  \n",
       "1      1  \n",
       "2      2  \n",
       "3      2  \n",
       "4      3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers = df[\"tokenizer\"].drop_duplicates()\n",
    "times = numpy.array([list(range(1, 10+1)) for tkr in tokenizers]).T.ravel()\n",
    "df[\"times\"] = times[:len(df)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行時間を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>times</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">valid_acc</th>\n",
       "      <th>1</th>\n",
       "      <td>0.942560</td>\n",
       "      <td>0.953867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.951153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.947535</td>\n",
       "      <td>0.954772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.960651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.954772</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.959294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.949796</td>\n",
       "      <td>0.953415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tokenizer        JpTokenizerMeCab  JpTokenizerSentencePiece\n",
       "          times                                            \n",
       "valid_acc 1              0.942560                  0.953867\n",
       "          2              0.953415                  0.951153\n",
       "          3              0.947535                  0.954772\n",
       "          4              0.952962                  0.960651\n",
       "          5              0.954772                  0.957033\n",
       "          6              0.953867                  0.957485\n",
       "          7              0.951153                  0.957485\n",
       "          8              0.946630                  0.959294\n",
       "          9              0.947987                  0.957033\n",
       "          10             0.949796                  0.953415"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_acc_df = df.pivot(index=\"tokenizer\", columns=\"times\", values=[\"valid_acc\", \"train_acc\", \"elapsed_time\", \"cpu_time\"]).T\n",
    "#_acc_df[\"mean\"] = pvdf.mean(axis=1)\n",
    "#_acc_df[\"std\"] = pvdf.std(axis=1)\n",
    "_acc_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経過時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>times</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <td>62.041894</td>\n",
       "      <td>61.877692</td>\n",
       "      <td>61.406188</td>\n",
       "      <td>61.731160</td>\n",
       "      <td>62.640263</td>\n",
       "      <td>61.855734</td>\n",
       "      <td>62.030530</td>\n",
       "      <td>61.769541</td>\n",
       "      <td>61.421218</td>\n",
       "      <td>61.370561</td>\n",
       "      <td>61.814478</td>\n",
       "      <td>0.362408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "      <td>94.967452</td>\n",
       "      <td>95.696913</td>\n",
       "      <td>93.747547</td>\n",
       "      <td>94.643323</td>\n",
       "      <td>94.188930</td>\n",
       "      <td>94.490407</td>\n",
       "      <td>93.761616</td>\n",
       "      <td>95.202418</td>\n",
       "      <td>93.966003</td>\n",
       "      <td>94.462001</td>\n",
       "      <td>94.512661</td>\n",
       "      <td>0.604838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "times                             1          2          3          4  \\\n",
       "tokenizer                                                              \n",
       "JpTokenizerMeCab          62.041894  61.877692  61.406188  61.731160   \n",
       "JpTokenizerSentencePiece  94.967452  95.696913  93.747547  94.643323   \n",
       "\n",
       "times                             5          6          7          8  \\\n",
       "tokenizer                                                              \n",
       "JpTokenizerMeCab          62.640263  61.855734  62.030530  61.769541   \n",
       "JpTokenizerSentencePiece  94.188930  94.490407  93.761616  95.202418   \n",
       "\n",
       "times                             9         10       mean       std  \n",
       "tokenizer                                                            \n",
       "JpTokenizerMeCab          61.421218  61.370561  61.814478  0.362408  \n",
       "JpTokenizerSentencePiece  93.966003  94.462001  94.512661  0.604838  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf = _acc_df.loc[\"elapsed_time\"].dropna().T\n",
    "edf[\"mean\"] = edf.mean(axis=1)\n",
    "edf[\"std\"] = edf.std(axis=1)\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab: 1.0 min (0.4 sec)\n",
      "JpTokenizerSentencePiece: 1.6 min (0.6 sec)\n"
     ]
    }
   ],
   "source": [
    "for tkr, m, s in edf[[\"mean\", \"std\"]].reset_index().values:\n",
    "    print(f\"{tkr}: {m/60:.1f} min ({s:.1f} sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>times</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <td>296.817731</td>\n",
       "      <td>294.313050</td>\n",
       "      <td>291.654511</td>\n",
       "      <td>293.421905</td>\n",
       "      <td>300.750381</td>\n",
       "      <td>292.933979</td>\n",
       "      <td>295.147034</td>\n",
       "      <td>293.366931</td>\n",
       "      <td>291.367480</td>\n",
       "      <td>291.590212</td>\n",
       "      <td>294.136321</td>\n",
       "      <td>2.735782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "      <td>549.502217</td>\n",
       "      <td>551.355981</td>\n",
       "      <td>540.871926</td>\n",
       "      <td>543.075450</td>\n",
       "      <td>542.472539</td>\n",
       "      <td>543.068041</td>\n",
       "      <td>541.148729</td>\n",
       "      <td>546.557152</td>\n",
       "      <td>542.139124</td>\n",
       "      <td>544.121128</td>\n",
       "      <td>544.431229</td>\n",
       "      <td>3.383975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "times                              1           2           3           4  \\\n",
       "tokenizer                                                                  \n",
       "JpTokenizerMeCab          296.817731  294.313050  291.654511  293.421905   \n",
       "JpTokenizerSentencePiece  549.502217  551.355981  540.871926  543.075450   \n",
       "\n",
       "times                              5           6           7           8  \\\n",
       "tokenizer                                                                  \n",
       "JpTokenizerMeCab          300.750381  292.933979  295.147034  293.366931   \n",
       "JpTokenizerSentencePiece  542.472539  543.068041  541.148729  546.557152   \n",
       "\n",
       "times                              9          10        mean       std  \n",
       "tokenizer                                                               \n",
       "JpTokenizerMeCab          291.367480  291.590212  294.136321  2.735782  \n",
       "JpTokenizerSentencePiece  542.139124  544.121128  544.431229  3.383975  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = _acc_df.loc[\"cpu_time\"].dropna().T\n",
    "cdf[\"mean\"] = cdf.mean(axis=1)\n",
    "cdf[\"std\"] = cdf.std(axis=1)\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab: 4.9 min (2.7 sec)\n",
      "JpTokenizerSentencePiece: 9.1 min (3.4 sec)\n"
     ]
    }
   ],
   "source": [
    "for tkr, m, s in cdf[[\"mean\", \"std\"]].reset_index().values:\n",
    "    print(f\"{tkr}: {m/60:.1f} min ({s:.1f} sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tokenizer</th>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942560</td>\n",
       "      <td>0.953867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.951153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.947535</td>\n",
       "      <td>0.954772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.960651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.954772</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.959294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.949796</td>\n",
       "      <td>0.953415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tokenizer  JpTokenizerMeCab  JpTokenizerSentencePiece\n",
       "times                                                \n",
       "1                  0.942560                  0.953867\n",
       "2                  0.953415                  0.951153\n",
       "3                  0.947535                  0.954772\n",
       "4                  0.952962                  0.960651\n",
       "5                  0.954772                  0.957033\n",
       "6                  0.953867                  0.957485\n",
       "7                  0.951153                  0.957485\n",
       "8                  0.946630                  0.959294\n",
       "9                  0.947987                  0.957033\n",
       "10                 0.949796                  0.953415"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = _acc_df.loc[\"valid_acc\"].dropna()\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>times</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.954772</td>\n",
       "      <td>0.960651</td>\n",
       "      <td>0.957033</td>\n",
       "      <td>0.957485</td>\n",
       "      <td>0.957485</td>\n",
       "      <td>0.959294</td>\n",
       "      <td>0.957033</td>\n",
       "      <td>0.953415</td>\n",
       "      <td>95.621891</td>\n",
       "      <td>0.273473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <td>0.942560</td>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.947535</td>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.954772</td>\n",
       "      <td>0.953867</td>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.949796</td>\n",
       "      <td>95.006784</td>\n",
       "      <td>0.369215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "times                            1         2         3         4         5  \\\n",
       "tokenizer                                                                    \n",
       "JpTokenizerSentencePiece  0.953867  0.951153  0.954772  0.960651  0.957033   \n",
       "JpTokenizerMeCab          0.942560  0.953415  0.947535  0.952962  0.954772   \n",
       "\n",
       "times                            6         7         8         9        10  \\\n",
       "tokenizer                                                                    \n",
       "JpTokenizerSentencePiece  0.957485  0.957485  0.959294  0.957033  0.953415   \n",
       "JpTokenizerMeCab          0.953867  0.951153  0.946630  0.947987  0.949796   \n",
       "\n",
       "times                          mean       std  \n",
       "tokenizer                                      \n",
       "JpTokenizerSentencePiece  95.621891  0.273473  \n",
       "JpTokenizerMeCab          95.006784  0.369215  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = acc_df.dropna().T.copy()\n",
    "acc[\"mean\"] = acc.mean(axis=1)\n",
    "acc[\"std\"] = acc.std(axis=1)\n",
    "acc[\"mean\"] *= 100\n",
    "acc[\"std\"] *= 100\n",
    "acc.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab: 95.0 % (0.4 %)\n",
      "JpTokenizerSentencePiece: 95.6 % (0.3 %)\n"
     ]
    }
   ],
   "source": [
    "for tkr, m, s in acc[[\"mean\", \"std\"]].reset_index().values:\n",
    "    print(f\"{tkr}: {m:.1f} % ({s:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規性の検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab 0.9387243390083313 0.5389084815979004 False\n",
      "JpTokenizerSentencePiece 0.9642882943153381 0.833436906337738 False\n"
     ]
    }
   ],
   "source": [
    "for tkr in acc_df.columns:\n",
    "    W, pvalue = scipy.stats.shapiro(acc_df[tkr].dropna())\n",
    "    print(tkr, W, pvalue, pvalue < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規乱数で検定に必要なサンプルサイズを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9159561395645142, 0.32445773482322693)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = numpy.random.normal(0, 1, 10)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94683438539505, 0.000516309926752001)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = numpy.random.uniform(0, 1, 100)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9455024600028992, 0.022365480661392212)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = numpy.random.uniform(0, 1, 50)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10 サンプル程度では、正規分布からのサンプルであることを否定するのは難しそう\n",
    "- 50サンプルで、ギリギリな印象\n",
    "- 結果的に、50-100サンプルは正規性を否定できるためのサンプルとして取得したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t検定（対応あり）\n",
    "- MeCab, SentencePiece の2群のみを比較するため、t検定でよい\n",
    "- t検定は、正規性に頑健性があるので、参考として実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab JpTokenizerSentencePiece -4.353253917718455 0.0018414918124858705 True\n",
      "JpTokenizerSentencePiece JpTokenizerMeCab 4.353253917718455 0.0018414918124858705 True\n"
     ]
    }
   ],
   "source": [
    "cols = acc_df.columns\n",
    "for base in cols:\n",
    "    for target in [trg for trg in cols if trg != base]:\n",
    "        t, pvalue = scipy.stats.ttest_rel(acc_df[base], acc_df[target])\n",
    "        if pvalue < 0.05:\n",
    "            print(base, target, t, pvalue, (pvalue < 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ウィルコクソンの符号順位検定\n",
    "- 両側検定\n",
    "- 連続補正なし（精度は、離散分布ではないため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab JpTokenizerSentencePiece 1.0 0.0069104298078147995 True\n",
      "JpTokenizerSentencePiece JpTokenizerMeCab 1.0 0.0069104298078147995 True\n"
     ]
    }
   ],
   "source": [
    "cols = acc_df.columns\n",
    "for base in cols:\n",
    "    for target in [trg for trg in cols if trg != base]:\n",
    "        w, pvalue = scipy.stats.wilcoxon(acc_df[base], acc_df[target], correction=False)\n",
    "        if pvalue < 0.05:\n",
    "            print(base, target, w, pvalue, (pvalue < 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検定結果\n",
    "\n",
    "- t検定も、ウィルコクソンの符号順位和検定のいずれも、有意差がある結果になった\n",
    "\n",
    "| tokenizer name | accuracy mean (std) |\n",
    "| --------------- | --- |\n",
    "| JpTokenizerMeCab | 95.0 (0.4) |\n",
    "| JpTokenizerSentencePiece | 95.6 (0.3) |\n",
    "\n",
    "- MeCab の平均が、$95.0 \\% (\\pm 0.4 \\%)$、SentencePiece の平均が $95.6 \\% (\\pm 0.3 \\%)$ \n",
    "    - 精度は、MeCab < SentencePiece\n",
    "    - 精度差は、偶然ではかなり発生しづらく（0.7%未満）、何らかの意味・理由があると言える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "- MeCab, SentencePiece の精度を比較すると、有意に、SentencePiece の方が(約0.6%)よい\n",
    "- 精度と実行時間の関係は、以下のようになる\n",
    "    \n",
    "| tokenizer name | accuracy mean (std) | elapsed time mean (std) | cpu time mean (std) |\n",
    "| -------------- | --- | ----------------------- | ------------------- |\n",
    "| JpTokenizerMeCab | 95.0 % (0.4 %) | 1.0 min (0.4 sec) | 4.9 min (2.7 sec) |\n",
    "| JpTokenizerSentencePiece | 95.6 % (0.3 %) | 1.6 min (0.6 sec) | 9.1 min (3.4 sec) |\n",
    "    \n",
    "- 経過時間（elapsed time）を、比較すると 約 0.6 min = 36 sec の差であった\n",
    "- CPU時間（cpu time）を、比較すると 約 4.9 min, 9.1 min と、倍近く差がある\n",
    "    - これは、SentencePiece が、マルチCPUで動作することが起因していると考えられる\n",
    "        - 故に、CPU時間が倍近くになっている\n",
    "    - MeCab 単体は、1 cpu で動作するが、SentencePiece の学習（fit()）は、8 cpu で動作することが起因すると考える\n",
    "    - 形態素解析(MeCab, SentencePiece)後のpipeline は、同じなので、いずれも途中から8cpu を利用する\n",
    "- 以上をまとめると\n",
    "    - 計算資源が十分（2 cpu 以上）ある場合は、経過時間の差は大きくない（いずれも実用に耐えうる）\n",
    "    - 計算資源が十分な場合は、若干だがより精度が高い SentencePiece を利用してよく\n",
    "    - 計算資源が1cpuに限られている場合で、経過時間を優先すべきときは、MeCab を利用した方が良さそうである\n",
    "        - 例： 1cpu だと、CPU時間≒経過時間になるため、倍ぐらいの時間差がでる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残課題\n",
    "\n",
    "- サンプルサイズが、$10 \\times 2=20$ と小さいので、$100 \\times 2=200$ 程度で評価しても差があるのか否かを評価する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
