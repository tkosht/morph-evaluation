{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeCab, SentencePiece の精度評価\n",
    "\n",
    "- 評価データセット：ldcc\n",
    "- 評価方法：pipeline\n",
    "    - ../model/\n",
    "        - pipe-jptokenizermecab.gz\n",
    "        - pipe-jptokenizersentencepiece.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from ldccset import DatasetLdcc\n",
    "from aozoraset import DatasetAozora\n",
    "from classify import TagDocMaker, Doc2Vectorizer\n",
    "from classify import JpTokenizerMeCab, JpTokenizerSentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelineの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 19.8 ms, total: 1.82 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import joblib\n",
    "from classify import ident_tokener, SparsetoDense, Transer\n",
    "try:\n",
    "    os.chdir(\"../\")\n",
    "    pipe_mecab = joblib.load(\"model/pipe-jptokenizermecab.gz\")\n",
    "    pipe_sentencepiece = joblib.load(\"model/pipe-jptokenizersentencepiece.gz\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    os.chdir(\"notebook/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tokenizer',\n",
       "                 <classify.JpTokenizerMeCab object at 0x7efd4c76f3c8>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True...\n",
       "                 LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                colsample_bytree=1.0, importance_type='gain',\n",
       "                                learning_rate=0.1, max_depth=-1,\n",
       "                                min_child_samples=20, min_child_weight=0.001,\n",
       "                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                                num_class=5, num_leaves=31, objective='softmax',\n",
       "                                random_state=None, reg_alpha=0.0,\n",
       "                                reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "                                subsample_for_bin=200000, subsample_freq=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tokenizer',\n",
       "                 <classify.JpTokenizerSentencePiece object at 0x7efd39453748>),\n",
       "                ('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_...\n",
       "                 LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                colsample_bytree=1.0, importance_type='gain',\n",
       "                                learning_rate=0.1, max_depth=-1,\n",
       "                                min_child_samples=20, min_child_weight=0.001,\n",
       "                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                                num_class=5, num_leaves=31, objective='softmax',\n",
       "                                random_state=None, reg_alpha=0.0,\n",
       "                                reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "                                subsample_for_bin=200000, subsample_freq=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>cpu_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944369</td>\n",
       "      <td>69.335296</td>\n",
       "      <td>332.607681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956128</td>\n",
       "      <td>110.367700</td>\n",
       "      <td>631.398006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952962</td>\n",
       "      <td>72.925654</td>\n",
       "      <td>350.740181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954319</td>\n",
       "      <td>115.859467</td>\n",
       "      <td>659.670910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>69.753902</td>\n",
       "      <td>334.857192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tokenizer  train_acc  valid_acc  elapsed_time    cpu_time\n",
       "0          JpTokenizerMeCab        1.0   0.944369     69.335296  332.607681\n",
       "1  JpTokenizerSentencePiece        1.0   0.956128    110.367700  631.398006\n",
       "2          JpTokenizerMeCab        1.0   0.952962     72.925654  350.740181\n",
       "3  JpTokenizerSentencePiece        1.0   0.954319    115.859467  659.670910\n",
       "4          JpTokenizerMeCab        1.0   0.945274     69.753902  334.857192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_csv = \"../data/result.csv\"\n",
    "columns = [\"tokenizer\", \"train_acc\", \"valid_acc\", \"elapsed_time\", \"cpu_time\"]\n",
    "df = pandas.read_csv(result_csv, header=None, names=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回数情報を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>cpu_time</th>\n",
       "      <th>times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944369</td>\n",
       "      <td>69.335296</td>\n",
       "      <td>332.607681</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956128</td>\n",
       "      <td>110.367700</td>\n",
       "      <td>631.398006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952962</td>\n",
       "      <td>72.925654</td>\n",
       "      <td>350.740181</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JpTokenizerSentencePiece</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954319</td>\n",
       "      <td>115.859467</td>\n",
       "      <td>659.670910</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JpTokenizerMeCab</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>69.753902</td>\n",
       "      <td>334.857192</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tokenizer  train_acc  valid_acc  elapsed_time    cpu_time  \\\n",
       "0          JpTokenizerMeCab        1.0   0.944369     69.335296  332.607681   \n",
       "1  JpTokenizerSentencePiece        1.0   0.956128    110.367700  631.398006   \n",
       "2          JpTokenizerMeCab        1.0   0.952962     72.925654  350.740181   \n",
       "3  JpTokenizerSentencePiece        1.0   0.954319    115.859467  659.670910   \n",
       "4          JpTokenizerMeCab        1.0   0.945274     69.753902  334.857192   \n",
       "\n",
       "   times  \n",
       "0      1  \n",
       "1      1  \n",
       "2      2  \n",
       "3      2  \n",
       "4      3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers = df[\"tokenizer\"].drop_duplicates()\n",
    "n = len(df) // 2\n",
    "times = numpy.array([list(range(1, n+1)) for tkr in tokenizers]).T.ravel()\n",
    "times\n",
    "df[\"times\"] = times[:len(df)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行時間を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>times</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">valid_acc</th>\n",
       "      <th>1</th>\n",
       "      <td>0.944369</td>\n",
       "      <td>0.956128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.955676</td>\n",
       "      <td>0.961104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956581</td>\n",
       "      <td>0.955224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.957938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.961556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.944821</td>\n",
       "      <td>0.960199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "tokenizer        JpTokenizerMeCab  JpTokenizerSentencePiece\n",
       "          times                                            \n",
       "valid_acc 1              0.944369                  0.956128\n",
       "          2              0.952962                  0.954319\n",
       "          3              0.945274                  0.954319\n",
       "          4              0.955676                  0.961104\n",
       "          5              0.956581                  0.955224\n",
       "          6              0.950249                  0.957938\n",
       "          7              0.953415                  0.957485\n",
       "          8              0.947987                  0.961556\n",
       "          9              0.947987                  0.957033\n",
       "          10             0.944821                  0.960199"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_acc_df = df.pivot(index=\"tokenizer\", columns=\"times\", values=[\"valid_acc\", \"train_acc\", \"elapsed_time\", \"cpu_time\"]).T\n",
    "#_acc_df[\"mean\"] = pvdf.mean(axis=1)\n",
    "#_acc_df[\"std\"] = pvdf.std(axis=1)\n",
    "_acc_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経過時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>times</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <td>69.335296</td>\n",
       "      <td>72.925654</td>\n",
       "      <td>69.753902</td>\n",
       "      <td>67.479392</td>\n",
       "      <td>67.771489</td>\n",
       "      <td>69.552905</td>\n",
       "      <td>75.482269</td>\n",
       "      <td>69.395602</td>\n",
       "      <td>72.837610</td>\n",
       "      <td>70.585096</td>\n",
       "      <td>...</td>\n",
       "      <td>62.292603</td>\n",
       "      <td>61.729877</td>\n",
       "      <td>62.531481</td>\n",
       "      <td>62.099945</td>\n",
       "      <td>62.240652</td>\n",
       "      <td>61.816332</td>\n",
       "      <td>62.112534</td>\n",
       "      <td>62.092931</td>\n",
       "      <td>65.023183</td>\n",
       "      <td>4.115273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "      <td>110.367700</td>\n",
       "      <td>115.859467</td>\n",
       "      <td>118.826758</td>\n",
       "      <td>104.146838</td>\n",
       "      <td>105.396650</td>\n",
       "      <td>107.318686</td>\n",
       "      <td>108.428783</td>\n",
       "      <td>115.120286</td>\n",
       "      <td>108.636047</td>\n",
       "      <td>111.911376</td>\n",
       "      <td>...</td>\n",
       "      <td>94.794646</td>\n",
       "      <td>94.912333</td>\n",
       "      <td>94.788782</td>\n",
       "      <td>95.611353</td>\n",
       "      <td>95.053526</td>\n",
       "      <td>95.421243</td>\n",
       "      <td>94.589187</td>\n",
       "      <td>95.824682</td>\n",
       "      <td>100.347440</td>\n",
       "      <td>7.686137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "times                              1           2           3           4  \\\n",
       "tokenizer                                                                  \n",
       "JpTokenizerMeCab           69.335296   72.925654   69.753902   67.479392   \n",
       "JpTokenizerSentencePiece  110.367700  115.859467  118.826758  104.146838   \n",
       "\n",
       "times                              5           6           7           8  \\\n",
       "tokenizer                                                                  \n",
       "JpTokenizerMeCab           67.771489   69.552905   75.482269   69.395602   \n",
       "JpTokenizerSentencePiece  105.396650  107.318686  108.428783  115.120286   \n",
       "\n",
       "times                              9          10  ...         93         94  \\\n",
       "tokenizer                                         ...                         \n",
       "JpTokenizerMeCab           72.837610   70.585096  ...  62.292603  61.729877   \n",
       "JpTokenizerSentencePiece  108.636047  111.911376  ...  94.794646  94.912333   \n",
       "\n",
       "times                            95         96         97         98  \\\n",
       "tokenizer                                                              \n",
       "JpTokenizerMeCab          62.531481  62.099945  62.240652  61.816332   \n",
       "JpTokenizerSentencePiece  94.788782  95.611353  95.053526  95.421243   \n",
       "\n",
       "times                            99        100        mean       std  \n",
       "tokenizer                                                             \n",
       "JpTokenizerMeCab          62.112534  62.092931   65.023183  4.115273  \n",
       "JpTokenizerSentencePiece  94.589187  95.824682  100.347440  7.686137  \n",
       "\n",
       "[2 rows x 102 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf = _acc_df.loc[\"elapsed_time\"].dropna().T\n",
    "edf[\"mean\"] = edf.mean(axis=1)\n",
    "edf[\"std\"] = edf.std(axis=1)\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab: 1.1 min (4.1 sec)\n",
      "JpTokenizerSentencePiece: 1.7 min (7.7 sec)\n"
     ]
    }
   ],
   "source": [
    "for tkr, m, s in edf[[\"mean\", \"std\"]].reset_index().values:\n",
    "    print(f\"{tkr}: {m/60:.1f} min ({s:.1f} sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>times</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <td>332.607681</td>\n",
       "      <td>350.740181</td>\n",
       "      <td>334.857192</td>\n",
       "      <td>324.195809</td>\n",
       "      <td>325.985670</td>\n",
       "      <td>334.925326</td>\n",
       "      <td>368.034112</td>\n",
       "      <td>332.416610</td>\n",
       "      <td>351.524826</td>\n",
       "      <td>341.340222</td>\n",
       "      <td>...</td>\n",
       "      <td>297.976241</td>\n",
       "      <td>295.077203</td>\n",
       "      <td>300.346705</td>\n",
       "      <td>297.323128</td>\n",
       "      <td>299.26722</td>\n",
       "      <td>295.326602</td>\n",
       "      <td>296.702732</td>\n",
       "      <td>297.005095</td>\n",
       "      <td>312.067686</td>\n",
       "      <td>21.099491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "      <td>631.398006</td>\n",
       "      <td>659.670910</td>\n",
       "      <td>672.109113</td>\n",
       "      <td>593.418019</td>\n",
       "      <td>597.599394</td>\n",
       "      <td>608.212748</td>\n",
       "      <td>614.086151</td>\n",
       "      <td>650.768025</td>\n",
       "      <td>618.193340</td>\n",
       "      <td>638.298616</td>\n",
       "      <td>...</td>\n",
       "      <td>547.341090</td>\n",
       "      <td>546.093395</td>\n",
       "      <td>546.962231</td>\n",
       "      <td>549.048228</td>\n",
       "      <td>548.50603</td>\n",
       "      <td>547.799480</td>\n",
       "      <td>544.533046</td>\n",
       "      <td>551.169484</td>\n",
       "      <td>575.390356</td>\n",
       "      <td>40.692919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "times                              1           2           3           4  \\\n",
       "tokenizer                                                                  \n",
       "JpTokenizerMeCab          332.607681  350.740181  334.857192  324.195809   \n",
       "JpTokenizerSentencePiece  631.398006  659.670910  672.109113  593.418019   \n",
       "\n",
       "times                              5           6           7           8  \\\n",
       "tokenizer                                                                  \n",
       "JpTokenizerMeCab          325.985670  334.925326  368.034112  332.416610   \n",
       "JpTokenizerSentencePiece  597.599394  608.212748  614.086151  650.768025   \n",
       "\n",
       "times                              9          10  ...          93          94  \\\n",
       "tokenizer                                         ...                           \n",
       "JpTokenizerMeCab          351.524826  341.340222  ...  297.976241  295.077203   \n",
       "JpTokenizerSentencePiece  618.193340  638.298616  ...  547.341090  546.093395   \n",
       "\n",
       "times                             95          96         97          98  \\\n",
       "tokenizer                                                                 \n",
       "JpTokenizerMeCab          300.346705  297.323128  299.26722  295.326602   \n",
       "JpTokenizerSentencePiece  546.962231  549.048228  548.50603  547.799480   \n",
       "\n",
       "times                             99         100        mean        std  \n",
       "tokenizer                                                                \n",
       "JpTokenizerMeCab          296.702732  297.005095  312.067686  21.099491  \n",
       "JpTokenizerSentencePiece  544.533046  551.169484  575.390356  40.692919  \n",
       "\n",
       "[2 rows x 102 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf = _acc_df.loc[\"cpu_time\"].dropna().T\n",
    "cdf[\"mean\"] = cdf.mean(axis=1)\n",
    "cdf[\"std\"] = cdf.std(axis=1)\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab: 5.2 min (21.1 sec)\n",
      "JpTokenizerSentencePiece: 9.6 min (40.7 sec)\n"
     ]
    }
   ],
   "source": [
    "for tkr, m, s in cdf[[\"mean\", \"std\"]].reset_index().values:\n",
    "    print(f\"{tkr}: {m/60:.1f} min ({s:.1f} sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>tokenizer</th>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.944369</td>\n",
       "      <td>0.956128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.955676</td>\n",
       "      <td>0.961104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956581</td>\n",
       "      <td>0.955224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.957938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.961556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.944821</td>\n",
       "      <td>0.960199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.962460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.943012</td>\n",
       "      <td>0.947535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.956128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.959294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.948440</td>\n",
       "      <td>0.955224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.943917</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.946178</td>\n",
       "      <td>0.950701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.946178</td>\n",
       "      <td>0.950701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.939846</td>\n",
       "      <td>0.955224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.949796</td>\n",
       "      <td>0.955676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.943917</td>\n",
       "      <td>0.953415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.958390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.956128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.943464</td>\n",
       "      <td>0.957485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.962460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.951153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.948440</td>\n",
       "      <td>0.958842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.941655</td>\n",
       "      <td>0.953867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.961104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.943917</td>\n",
       "      <td>0.950249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.959747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.943012</td>\n",
       "      <td>0.957938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.943012</td>\n",
       "      <td>0.956128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.951606</td>\n",
       "      <td>0.952510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.947535</td>\n",
       "      <td>0.961556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.952058</td>\n",
       "      <td>0.960199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.950701</td>\n",
       "      <td>0.950249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.944821</td>\n",
       "      <td>0.953867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.952058</td>\n",
       "      <td>0.949344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.943917</td>\n",
       "      <td>0.947535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.958842</td>\n",
       "      <td>0.957033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.951153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.956128</td>\n",
       "      <td>0.961556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.957938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.957485</td>\n",
       "      <td>0.960199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.957938</td>\n",
       "      <td>0.960651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.952058</td>\n",
       "      <td>0.951606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.949344</td>\n",
       "      <td>0.953415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.950701</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.960651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.944821</td>\n",
       "      <td>0.947987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.956128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.943917</td>\n",
       "      <td>0.952510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.948892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.935323</td>\n",
       "      <td>0.951153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.946178</td>\n",
       "      <td>0.954319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.947083</td>\n",
       "      <td>0.954772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.951606</td>\n",
       "      <td>0.959294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.952510</td>\n",
       "      <td>0.964722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "tokenizer  JpTokenizerMeCab  JpTokenizerSentencePiece\n",
       "times                                                \n",
       "1                  0.944369                  0.956128\n",
       "2                  0.952962                  0.954319\n",
       "3                  0.945274                  0.954319\n",
       "4                  0.955676                  0.961104\n",
       "5                  0.956581                  0.955224\n",
       "...                     ...                       ...\n",
       "96                 0.935323                  0.951153\n",
       "97                 0.946178                  0.954319\n",
       "98                 0.947083                  0.954772\n",
       "99                 0.951606                  0.959294\n",
       "100                0.952510                  0.964722\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = _acc_df.loc[\"valid_acc\"].dropna()\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>times</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JpTokenizerSentencePiece</th>\n",
       "      <td>0.956128</td>\n",
       "      <td>0.954319</td>\n",
       "      <td>0.954319</td>\n",
       "      <td>0.961104</td>\n",
       "      <td>0.955224</td>\n",
       "      <td>0.957938</td>\n",
       "      <td>0.957485</td>\n",
       "      <td>0.961556</td>\n",
       "      <td>0.957033</td>\n",
       "      <td>0.960199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956128</td>\n",
       "      <td>0.952510</td>\n",
       "      <td>0.948892</td>\n",
       "      <td>0.951153</td>\n",
       "      <td>0.954319</td>\n",
       "      <td>0.954772</td>\n",
       "      <td>0.959294</td>\n",
       "      <td>0.964722</td>\n",
       "      <td>95.596110</td>\n",
       "      <td>0.391064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JpTokenizerMeCab</th>\n",
       "      <td>0.944369</td>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.955676</td>\n",
       "      <td>0.956581</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.953415</td>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.947987</td>\n",
       "      <td>0.944821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945274</td>\n",
       "      <td>0.943917</td>\n",
       "      <td>0.946630</td>\n",
       "      <td>0.935323</td>\n",
       "      <td>0.946178</td>\n",
       "      <td>0.947083</td>\n",
       "      <td>0.951606</td>\n",
       "      <td>0.952510</td>\n",
       "      <td>94.869742</td>\n",
       "      <td>0.480086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "times                            1         2         3         4         5  \\\n",
       "tokenizer                                                                    \n",
       "JpTokenizerSentencePiece  0.956128  0.954319  0.954319  0.961104  0.955224   \n",
       "JpTokenizerMeCab          0.944369  0.952962  0.945274  0.955676  0.956581   \n",
       "\n",
       "times                            6         7         8         9        10  \\\n",
       "tokenizer                                                                    \n",
       "JpTokenizerSentencePiece  0.957938  0.957485  0.961556  0.957033  0.960199   \n",
       "JpTokenizerMeCab          0.950249  0.953415  0.947987  0.947987  0.944821   \n",
       "\n",
       "times                     ...        93        94        95        96  \\\n",
       "tokenizer                 ...                                           \n",
       "JpTokenizerSentencePiece  ...  0.956128  0.952510  0.948892  0.951153   \n",
       "JpTokenizerMeCab          ...  0.945274  0.943917  0.946630  0.935323   \n",
       "\n",
       "times                           97        98        99       100       mean  \\\n",
       "tokenizer                                                                     \n",
       "JpTokenizerSentencePiece  0.954319  0.954772  0.959294  0.964722  95.596110   \n",
       "JpTokenizerMeCab          0.946178  0.947083  0.951606  0.952510  94.869742   \n",
       "\n",
       "times                          std  \n",
       "tokenizer                           \n",
       "JpTokenizerSentencePiece  0.391064  \n",
       "JpTokenizerMeCab          0.480086  \n",
       "\n",
       "[2 rows x 102 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = acc_df.dropna().T.copy()\n",
    "m = acc.mean(axis=1)\n",
    "s = acc.std(axis=1)\n",
    "acc[\"mean\"] = m\n",
    "acc[\"std\"] = s\n",
    "acc[\"mean\"] *= 100\n",
    "acc[\"std\"] *= 100\n",
    "acc.sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab: 94.9 % (0.5 %)\n",
      "JpTokenizerSentencePiece: 95.6 % (0.4 %)\n"
     ]
    }
   ],
   "source": [
    "for tkr, m, s in acc[[\"mean\", \"std\"]].reset_index().values:\n",
    "    print(f\"{tkr}: {m:.1f} % ({s:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規性の検定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab 0.9869959950447083 0.43735480308532715 False 非棄却\n",
      "JpTokenizerSentencePiece 0.9907791614532471 0.727403998374939 False 非棄却\n"
     ]
    }
   ],
   "source": [
    "for tkr in acc_df.columns:\n",
    "    W, pvalue = scipy.stats.shapiro(acc_df[tkr].dropna())\n",
    "    print(tkr, W, pvalue, pvalue < 0.05, \"棄却\" if pvalue < 0.05 else \"非棄却\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規乱数で検定に必要なサンプルサイズを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8561763763427734, 0.06876995414495468)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正規乱数 サンプルサイズ=10\n",
    "x = numpy.random.normal(0, 1, 10)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9917338490486145, 0.8016242980957031)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正規乱数 サンプルサイズ=100\n",
    "x = numpy.random.normal(0, 1, 100)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8837308287620544, 0.14397722482681274)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一様乱数 サンプルサイズ=10\n",
    "x = numpy.random.uniform(0, 1, 10)\n",
    "scipy.stats.shapiro(x)  # <- 棄却できず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9426774382591248, 0.017216842621564865)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一様乱数 サンプルサイズ=50\n",
    "x = numpy.random.uniform(0, 1, 50)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9250912666320801, 2.6490444724913687e-05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一様乱数 サンプルサイズ=100\n",
    "x = numpy.random.uniform(0, 1, 100)\n",
    "scipy.stats.shapiro(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- サンプルサイズ=10 では、正規分布からのサンプルであることを否定するのは難しそう\n",
    "    - サンプルサイズ=100 でやり直した\n",
    "    - やり直した結果、正規性は棄却されなかった\n",
    "        - i.e. 正規性があると考えても(測定データと)矛盾しない\n",
    "- 50サンプルで、ギリギリな印象\n",
    "- 結果的に、50-100サンプルは正規性を否定できるためのサンプルとして取得したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t検定（対応あり）\n",
    "- MeCab, SentencePiece の2群のみを比較するため、t検定でよい\n",
    "- t検定は、正規性に頑健性があるので、参考として実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab JpTokenizerSentencePiece -16.263633862651595 1.0211749301055299e-29 True\n",
      "JpTokenizerSentencePiece JpTokenizerMeCab 16.263633862651595 1.0211749301055299e-29 True\n"
     ]
    }
   ],
   "source": [
    "cols = acc_df.columns\n",
    "for base in cols:\n",
    "    for target in [trg for trg in cols if trg != base]:\n",
    "        t, pvalue = scipy.stats.ttest_rel(acc_df[base], acc_df[target])\n",
    "        if pvalue < 0.05:\n",
    "            print(base, target, t, pvalue, (pvalue < 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ウィルコクソンの符号順位検定\n",
    "- 両側検定\n",
    "- 連続補正なし（精度は、離散分布ではないため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JpTokenizerMeCab JpTokenizerSentencePiece 37.5 1.1971604369766303e-17 True\n",
      "JpTokenizerSentencePiece JpTokenizerMeCab 37.5 1.1971604369766303e-17 True\n"
     ]
    }
   ],
   "source": [
    "cols = acc_df.columns\n",
    "for base in cols:\n",
    "    for target in [trg for trg in cols if trg != base]:\n",
    "        w, pvalue = scipy.stats.wilcoxon(acc_df[base], acc_df[target], correction=False)\n",
    "        if pvalue < 0.05:\n",
    "            print(base, target, w, pvalue, (pvalue < 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検定結果\n",
    "\n",
    "- t検定も、ウィルコクソンの符号順位和検定のいずれも、有意差がある結果になった\n",
    "\n",
    "| tokenizer name | accuracy mean (std) |\n",
    "| --------------- | --- |\n",
    "| JpTokenizerMeCab | 94.9 (0.5) |\n",
    "| JpTokenizerSentencePiece | 95.6 (0.4) |\n",
    "\n",
    "- MeCab の平均が、$94.9 \\% (\\pm 0.5 \\%)$、SentencePiece の平均が $95.6 \\% (\\pm 0.4 \\%)$ \n",
    "    - 精度は、MeCab < SentencePiece\n",
    "    - 精度差は、偶然ではかなり発生しづらく（0.7%未満）、何らかの意味・理由があると言える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "- MeCab, SentencePiece の精度を比較すると、有意に、SentencePiece の方が(約0.6%)よい\n",
    "- 精度と実行時間の関係は、以下のようになる\n",
    "    \n",
    "| tokenizer name | accuracy mean (std) | elapsed time mean (std) | cpu time mean (std) |\n",
    "| -------------- | --- | ----------------------- | ------------------- |\n",
    "| JpTokenizerMeCab | 94.9 % (0.5 %) | 1.0 min (0.6 sec) | 4.9 min (4.0 sec) |\n",
    "| JpTokenizerSentencePiece | 95.6 % (0.4 %) | 1.6 min (0.7 sec) | 9.0 min (4.8 sec) |\n",
    "\n",
    "\n",
    "- 経過時間（elapsed time）を、比較すると 約 0.6 min = 36 sec の差であった\n",
    "- CPU時間（cpu time）を、比較すると 約 4.9 min, 9.0 min と、倍近く差がある\n",
    "    - これは、SentencePiece が、マルチCPUで動作することが起因していると考えられる\n",
    "        - 故に、CPU時間が倍近くになっている\n",
    "    - MeCab 単体は、1 cpu で動作するが、SentencePiece の学習（fit()）は、8 cpu で動作することがCPU時間に影響を与えていると考える\n",
    "    - 形態素解析(MeCab, SentencePiece)後のpipeline は、同じである（いずれも途中から8cpu を利用する）\n",
    "- 以上をまとめると\n",
    "    - 計算資源が十分（2 cpu 以上）ある場合は、経過時間の差は大きくない（いずれも実用に耐えうる）\n",
    "    - 計算資源が十分な場合は、若干だがより精度が高い SentencePiece を利用してよく\n",
    "    - 計算資源が1cpuに限られている場合で、経過時間を優先すべきときは、MeCab を利用した方が良さそうである\n",
    "        - 例： 1cpu だと、CPU時間≒経過時間になるため、倍ぐらいの時間差がでる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
